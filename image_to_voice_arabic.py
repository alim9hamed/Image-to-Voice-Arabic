# -*- coding: utf-8 -*-
"""image_to_voice_arabic.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Yr3bmW8vzYLbxAwT67OgM7jhG6cx1KvN

# **Image to text**

## Phi 3.5 Vision
"""

from IPython.display import Markdown, display
from PIL import Image
import requests
from transformers import AutoModelForCausalLM, AutoProcessor
import gc
import torch

class Img2Text:
    def __init__(self, device):
        self.device = device
        self.model_id = "microsoft/Phi-3.5-vision-instruct"
        self.model = AutoModelForCausalLM.from_pretrained(self.model_id,
                                                          device_map=self.device,
                                                          trust_remote_code=True,
                                                          torch_dtype="auto",
                                                          _attn_implementation='eager'
                                                        )
        self.processor = AutoProcessor.from_pretrained(self.model_id, trust_remote_code=True)
        self.messages = [{"role": "user", "content": "<|image_1|> Describe the image in Details"}]
        self.prompt = self.processor.tokenizer.apply_chat_template(self.messages, tokenize=False, add_generation_prompt=True)

    def run(self, image, messages=False):
        # Load the image using PIL if it is a path or URL
        if isinstance(image, str):
            image = Image.open(image).convert("RGB")
        # If image is not a list, make it a list containing the image
        if not isinstance(image, list):
            image = [image]
        if messages:
            self.prompt = self.processor.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

        inputs = self.processor(self.prompt, image, return_tensors="pt").to(self.device)
        generation_args = {"max_new_tokens": 1000, "temperature": 1, "do_sample": False}
        generate_ids = self.model.generate(**inputs, eos_token_id=self.processor.tokenizer.eos_token_id, **generation_args)
        generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]
        response = self.processor.batch_decode(generate_ids,
                                  skip_special_tokens=True,
                                  clean_up_tokenization_spaces=False)[0]
        del inputs
        del generate_ids
        torch.cuda.empty_cache()
        return response

    def cleanup(self):
        del self.model
        del self.processor
        torch.cuda.empty_cache()
        gc.collect()

Imgt2Text = Img2Text("cuda")
image = "/content/360_F_503362352_Q4oLkiACXRUv0uKVIonzo525a78Jf6d2.jpg"
Img2Text.run(image)

Img2Text.cleanup()

"""# **English to Arabic Translation**

## llama3
"""

!pip install groq -q
from groq import Groq

class Trans2Arabic:
    def __init__(self):
        self.api_key = "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"
        self.inst_prompt = """You are a skilled translator with extensive experience in English and Arabic translations.
                            You possess a deep understanding of the linguistic, cultural, and contextual nuances essential for accurate and effective translation between these languages. Highly motivated and detail-oriented, you are committed to delivering translations that maintain the integrity and intent of the original text.
                            Your role is crucial in ensuring clear and precise communication in our multilingual system.
                            Do not add anything other than the description"""

    def run(self, text):
        client = Groq(api_key=self.api_key)
        chat_completion = client.chat.completions.create(
            messages=[
                {
                    "role": "user",
                    "content": f"{self.inst_prompt} text to translate: {text}",
                }
            ],
            model="llama3-70b-8192",
            temperature=0,
        )

        return chat_completion.choices[0].message.content

translator = Trans2Arabic()
text = "The image captures a heartwarming scene of a family of foxes in their natural habitat. The mother fox, with her rich brown fur, stands tall and proud, her gaze fixed on the camera. She is flanked by her two cubs, their fur a lighter shade of brown, their eyes wide with curiosity and wonder. The cubs are exploring the grassy terrain, their small paws leaving imprints on the soft earth. The background is a blur of green, suggesting a dense forest or a grassy field, providing a safe and secluded environment for the family. The image is taken from a low angle, making the foxes appear larger and more majestic. The lighting is soft and natural, casting a warm glow on the scene and highlighting the intricate details of the foxes' fur and features. The image does not contain any text."

translated_text = translator.run(text)
print(translated_text)

"""# Text to Speech TTS Arabic

## TTs using Arabic tts

## TTs using xtts
"""

!pip install TTS
import torch
from TTS.api import TTS

class T2S_AR:
    def __init__(self, clone_voice_path, device="cuda:0"):
        self.clone_voice_path = clone_voice_path
        self.out_path = "output.wav"
        self.device = device
        self.tts = TTS("tts_models/multilingual/multi-dataset/xtts_v2").to(self.device)

    def run(self, text):
        self.tts.tts_to_file(text=text, speaker_wav=self.clone_voice_path, language="ar", file_path=self.out_path)
        return self.out_path

"""# Pipline"""

class ImgToSpeechAR:
    def __init__(self):
        self.img2text = Img2Text("cuda")
        self.translator = Trans2Arabic()
        self.path = "/content/speaker.opus"
        self.T2S_ar = T2S_AR(self.path)

    def convert_Img_to_voice(self, img):
        img_des = self.img2text.run(img)
        self.img2text.cleanup()
        img_des_ar = self.translator.run(img_des)
        speech_ar_output = self.T2S_ar.run(img_des_ar)
        return speech_ar_output

"""# Gradio"""

# import torch

# # Specify the device
# device = torch.device('cuda:0')

# # Clear cache on that specific device
# with torch.cuda.device(device):
#     torch.cuda.empty_cache()

!pip install gradio -q
import gradio as gr
# Initialize the class
pipline = ImgToSpeechAR()

# Define the Gradio interface
def gradio_interface(image):
    audio_path = pipline.convert_Img_to_voice(image)
    return audio_path

# Create the Gradio interface
iface = gr.Interface(
    fn=gradio_interface,

    inputs=gr.Image(type="pil", label="Upload Image"),
    outputs=gr.Audio(label="Generated Audio"),
    title="Speech to Image Converter",
    description="Upload an image, and the model will generate a speech description in Arabic.",
)

# Launch the interface
iface.launch(share=True, debug=True)